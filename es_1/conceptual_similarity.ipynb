{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.nltk.org/howto/wordnet.html WordNet examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_hypernyms(synset):\n",
    "    # get all hypernyms of a synset until the root of wordnet\n",
    "    # estendere per far prendere più iperonimi, non solo il primo ogni volta\n",
    "    ret_list = []\n",
    "    hypernyms = synset.hypernyms()\n",
    "    while hypernyms:\n",
    "        for hyper in hypernyms:\n",
    "            ret_list.append(hyper)\n",
    "        hypernyms = hypernyms[0].hypernyms()\n",
    "    return ret_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowest_common_subsumer(syn1, syn2): \n",
    "    return syn1.lowest_common_hypernyms(syn2)[0] if syn1.lowest_common_hypernyms(syn2) else None\n",
    "\n",
    "def lowest_common_subsumer_2(syn1, syn2):\n",
    "    # risale la gerarchia degli iperonimi, scegliendo sempre e solo il primo synset percjè non ho un PC non della NASA\n",
    "    syn1_hypernyms = get_all_hypernyms(syn1)\n",
    "    syn2_hypernyms = get_all_hypernyms(syn2)\n",
    "\n",
    "    for h1 in syn1_hypernyms:\n",
    "        if h1 in syn2_hypernyms:\n",
    "            return h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth(syn):\n",
    "    return syn.min_depth() if syn else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_path(): # restituisce sempre 19, per velocizzare le esecuzioni salvo il valore in una costante\n",
    "    max_path = 0\n",
    "    for synset in wn.all_synsets():\n",
    "        if synset.max_depth() > max_path:\n",
    "            max_path = synset.max_depth()\n",
    "    return max_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length(syn1, syn2): # NB, non esistono i cammini tra nomi e verbi in WordNet, pepr cui vanno rimossi i verbi credo\n",
    "    return syn1.shortest_path_distance(syn2) if syn1.shortest_path_distance(syn2) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PATH = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('carnivore.n.01')\n",
      "Synset('carnivore.n.01')\n"
     ]
    }
   ],
   "source": [
    "print(lowest_common_subsumer(wn.synset('dog.n.01'), wn.synset('big_cat.n.01')))\n",
    "print(lowest_common_subsumer_2(wn.synset('dog.n.01'), wn.synset('big_cat.n.01')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wu & Palmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wu_palmer(syn1, syn2):\n",
    "    dep = 0\n",
    "\n",
    "    lcs = lowest_common_subsumer(syn1, syn2)\n",
    "    dep = (depth(syn1) + depth(syn2))\n",
    "    \n",
    "    if dep == 0:\n",
    "        dep = 0.001\n",
    "        \n",
    "    return 2 * depth(lcs) / dep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortest Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_path(syn1, syn2):\n",
    "    return 2 * MAX_PATH - length(syn1, syn2) if length(syn1, syn2) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leakcock & Chodorow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leakcock_chodorow(syn1, syn2):\n",
    "    return -np.log(length(syn1, syn2) / 2 * MAX_PATH) if length(syn1, syn2) else -1000 #-1000 per indicare un valore di somiglianza basso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read lines from WordSim353.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "with open(r'C:\\Users\\andre\\Desktop\\Università\\Magistrale\\TLN\\PART 2\\TLN-LAB-PART-2\\data\\WordSim353.csv', 'r') as f:\n",
    "    word_sim = f.readlines()[1:]\n",
    "    for tuple in word_sim:\n",
    "        dataset.append(tuple.split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get synset from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns_1 = []\n",
    "syns_2 = []\n",
    "\n",
    "for tuple in dataset:\n",
    "    syns_1.append(wn.synsets(tuple[0]))\n",
    "    syns_2.append(wn.synsets(tuple[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute similarity using the three methods described above over all the combinations of synsets of every word in the input file.\n",
    "For each couple, take the maximum value of each similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9090909090909091, 0.9629629629629629, 1.0, 0.8571428571428571, 0.8, 0.5882352941176471, 0.7368421052631579, 0.75, 0, 0.9, 0.8, 0, 0.9090909090909091, 0.8571428571428571, 0.8, 0.625, 0.46153846153846156, 0, 0, 0.5882352941176471, 0.5333333333333333, 0.75, 0.8, 0.8, 0.5714285714285714, 0, 0, 0.6666666666666666, 0.75, 0.5, 1.0, 0.8571428571428571, 0.5263157894736842, 0.625, 1.0, 0.9, 0.7692307692307693, 0.6666666666666666, 0.23529411764705882, 0.18181818181818182, 0.8333333333333334, 0, 0.9473684210526315, 0.8888888888888888, 0.7777777777777778, 0.5555555555555556, 0, 0.5454545454545454, 0.7692307692307693, 0, 0.5454545454545454, 0.3157894736842105, 0.5, 0.6153846153846154, 0.125, 0.8888888888888888, 0.2857142857142857, 0.6, 0.875, 0.875, 0.16666666666666666, 0.36363636363636365, 0.4444444444444444, 0, 0.7142857142857143, 0.5454545454545454, 1.0, 1.0, 0.9473684210526315, 0.9090909090909091, 0.8888888888888888, 0.9473684210526315, 1.0, 1.0, 0.47058823529411764, 0.3076923076923077, 0.9473684210526315, 0.8571428571428571, 0.9230769230769231, 0.9230769230769231, 0.7142857142857143, 0.6, 0, 0.46153846153846156, 0.3333333333333333, 0.11764705882352941, 0.6, 0.3333333333333333, 0.5, 0.6, 0.4444444444444444, 0.6, 0.2857142857142857, 0.5, 0.15384615384615385, 0, 0.75, 0.8571428571428571, 0.9230769230769231, 0.9090909090909091, 0.8, 0.6666666666666666, 0.5, 0.8571428571428571, 0.16666666666666666, 0.14285714285714285, 0.2, 0.9285714285714286, 0.9230769230769231, 0.88, 0.782608695652174, 1.0, 1.1111111111111112, 1.0, 0.6, 0.7, 0.14285714285714285, 0.14285714285714285, 0.26666666666666666, 0.13333333333333333, 0.25, 0, 0.5333333333333333, 0.15384615384615385, 0.9333333333333333, 0.8571428571428571, 0.5454545454545454, 0.8333333333333334, 0.7272727272727273, 0.7692307692307693, 0.7692307692307693, 0.7142857142857143, 0.5, 0.5454545454545454, 0.9090909090909091, 0.8, 0.6666666666666666, 0.8888888888888888, 0.75, 0.5714285714285714, 0.5454545454545454, 0.7058823529411765, 1.0, 0.8333333333333334, 0.7272727272727273, 0.4444444444444444, 0, 0.7142857142857143, 0.6153846153846154, 0.5, 0.7142857142857143, 0.9629629629629629, 0.2727272727272727, 0.16666666666666666, 0, 0.18181818181818182, 0.4, 0.16666666666666666, 0.14285714285714285, 0.125, 0.8, 0.42857142857142855, 0.5, 0.9090909090909091, 0.26666666666666666, 0.6666666666666666, 0.25, 0.15384615384615385, 0.7142857142857143, 0.15384615384615385, 0.7692307692307693, 0, 0.5454545454545454, 0.42857142857142855, 0.13333333333333333, 0.18181818181818182, 0.8888888888888888, 0.6153846153846154, 0.7272727272727273, 0.4, 0, 0.6153846153846154, 0.15384615384615385, 0.8, 0.2222222222222222, 0.6153846153846154, 0.46153846153846156, 0.5454545454545454, 0.18181818181818182, 0.5454545454545454, 0.18181818181818182, 0.5, 0.6666666666666666, 0.75, 0.5714285714285714, 0.6, 0.18181818181818182, 0.4, 0.46153846153846156, 0.46153846153846156, 0.7272727272727273, 0.25, 0.15384615384615385, 0.15384615384615385, 0.5714285714285714, 0.8571428571428571, 0.5, 0.46153846153846156, 0.14285714285714285, 0.875, 0.15384615384615385, 0.4444444444444444, 0.16666666666666666, 0, 0.8333333333333334, 0.8333333333333334, 0.4, 0.15384615384615385, 0, 0.5714285714285714, 0.6666666666666666, 0.8, 0.14285714285714285, 0.6153846153846154, 0.42857142857142855, 0, 0.18181818181818182, 0, 0.5333333333333333, 0.5454545454545454, 0.6666666666666666, 0.2857142857142857, 0, 0.625, 0.625, 0, 0.9411764705882353, 0.5, 0.8235294117647058, 0.5333333333333333, 0.5714285714285714, 0.6666666666666666, 0.3076923076923077, 0.625, 0.5454545454545454, 0.9411764705882353, 0.6153846153846154, 0.5, 0.23529411764705882, 0.2857142857142857, 0.8888888888888888, 0.5333333333333333, 0.7272727272727273, 0.6153846153846154, 0.4, 0.875, 0.16666666666666666, 1.0, 0.18181818181818182, 0.15384615384615385, 0.5714285714285714, 0.4, 0.5454545454545454, 0.16666666666666666, 0.6666666666666666, 0.7692307692307693, 1.0, 0.18181818181818182, 0.2, 0, 0.7692307692307693, 0.8571428571428571, 0.5454545454545454, 0, 0.36363636363636365, 0.2222222222222222, 0, 1.0, 0.125, 0.5, 0.18181818181818182, 0.4444444444444444, 0.8333333333333334, 0.5, 0.3076923076923077, 0.2222222222222222, 0.8888888888888888, 0.8333333333333334, 0.7272727272727273, 0.2857142857142857, 0.5, 0.5, 0.4444444444444444, 0.5, 0.875, 0.7368421052631579, 0.8333333333333334, 0.16666666666666666, 0.6666666666666666, 0.7272727272727273, 0.7272727272727273, 0.2, 0.8, 0.4444444444444444, 0.9090909090909091, 0.6666666666666666, 1.25, 1.1111111111111112, 0.9090909090909091, 0.13333333333333333, 0.18181818181818182, 0.3333333333333333, 0.46153846153846156, 0.2222222222222222, 0.75, 0.4, 0.4444444444444444, 0, 0.7, 0.6, 0.4444444444444444, 0.7777777777777778, 0.36363636363636365, 0.47058823529411764, 0.18181818181818182, 0.16666666666666666, 0.16666666666666666, 0.8, 0.46153846153846156, 0.5454545454545454, 0.5454545454545454, 0.4, 0, 0.7142857142857143, 0.5454545454545454, 0.7692307692307693, 0.4444444444444444, 0.9090909090909091, 0.4, 0.3333333333333333, 0, 0.8333333333333334, 0.5714285714285714, 0.6666666666666666, 0.6, 0.5333333333333333, 0.5, 0.6666666666666666, 0.6666666666666666, 0, 0.42857142857142855, 0.47058823529411764, 0.18181818181818182]\n",
      "[37, 37, 27, 36, 35, 31, 32, 33, 28, 36, 35, 29, 36, 35, 36, 32, 31, 25, 25, 33, 32, 32, 33, 33, 32, 27, 33, 33, 36, 32, 32, 36, 26, 29, 36, 36, 34, 32, 23, 29, 36, 0, 37, 36, 33, 30, 26, 33, 35, 28, 31, 25, 28, 32, 24, 36, 33, 34, 36, 36, 28, 31, 32, 36, 34, 33, 34, 31, 37, 37, 37, 37, 34, 0, 29, 29, 37, 35, 37, 37, 34, 34, 21, 31, 30, 23, 34, 30, 34, 34, 33, 34, 28, 31, 27, 15, 34, 36, 37, 37, 36, 35, 32, 36, 28, 26, 30, 36, 36, 35, 33, 35, 36, 35, 31, 32, 26, 26, 27, 25, 26, 19, 32, 27, 37, 36, 33, 36, 35, 35, 35, 31, 32, 33, 37, 36, 35, 37, 36, 35, 33, 33, 36, 34, 35, 33, 33, 34, 33, 32, 33, 37, 22, 28, 25, 29, 30, 28, 26, 24, 35, 34, 29, 37, 27, 34, 32, 27, 36, 27, 35, 28, 33, 30, 26, 29, 37, 33, 35, 32, 27, 33, 27, 36, 31, 32, 31, 33, 29, 33, 29, 32, 34, 34, 32, 34, 29, 28, 31, 29, 35, 32, 27, 27, 32, 36, 30, 31, 26, 36, 27, 31, 28, 28, 36, 36, 30, 27, 25, 32, 34, 35, 26, 33, 31, 27, 29, 25, 31, 33, 35, 28, 25, 34, 32, 27, 37, 29, 35, 31, 32, 35, 30, 32, 33, 37, 33, 29, 26, 30, 36, 31, 35, 32, 32, 36, 28, 34, 29, 25, 30, 32, 33, 28, 33, 35, 32, 29, 30, 26, 35, 36, 30, 24, 31, 31, 0, 37, 24, 32, 29, 28, 36, 32, 30, 31, 37, 36, 35, 28, 32, 30, 33, 35, 36, 32, 36, 28, 34, 35, 35, 30, 36, 33, 37, 35, 36, 34, 36, 25, 29, 31, 32, 31, 34, 26, 28, 28, 32, 34, 33, 34, 31, 33, 29, 28, 28, 36, 31, 33, 33, 30, 30, 35, 33, 35, 33, 37, 33, 30, 26, 36, 32, 33, 34, 32, 32, 32, 34, 25, 30, 29, 29]\n",
      "[-2.2512917986064953, -2.2512917986064953, -4.6491870714048655, -2.9444389791664403, -3.349904087274605, -4.197201947661808, -4.04305126783455, -3.8607297110405954, -4.553876891600541, -2.9444389791664403, -3.349904087274605, -4.448516375942715, -2.9444389791664403, -3.349904087274605, -2.9444389791664403, -4.04305126783455, -4.197201947661808, -4.816241156068032, -4.816241156068032, -3.8607297110405954, -4.04305126783455, -4.04305126783455, -3.8607297110405954, -3.8607297110405954, -4.04305126783455, -4.6491870714048655, -3.8607297110405954, -3.8607297110405954, -2.9444389791664403, -4.04305126783455, -4.04305126783455, -2.9444389791664403, -4.736198448394496, -4.448516375942715, -2.9444389791664403, -2.9444389791664403, -3.6375861597263857, -4.04305126783455, -4.959341999708705, -4.448516375942715, -2.9444389791664403, -1000, -2.2512917986064953, -2.9444389791664403, -3.8607297110405954, -4.330733340286331, -4.736198448394496, -3.8607297110405954, -3.349904087274605, -4.553876891600541, -4.197201947661808, -4.816241156068032, -4.553876891600541, -4.04305126783455, -4.890349128221754, -2.9444389791664403, -3.8607297110405954, -3.6375861597263857, -2.9444389791664403, -2.9444389791664403, -4.553876891600541, -4.197201947661808, -4.04305126783455, -2.9444389791664403, -3.6375861597263857, -3.8607297110405954, -3.6375861597263857, -4.197201947661808, -2.2512917986064953, -2.2512917986064953, -2.2512917986064953, -2.2512917986064953, -3.6375861597263857, -1000, -4.448516375942715, -4.448516375942715, -2.2512917986064953, -3.349904087274605, -2.2512917986064953, -2.2512917986064953, -3.6375861597263857, -3.6375861597263857, -5.084505142662711, -4.197201947661808, -4.330733340286331, -4.959341999708705, -3.6375861597263857, -4.330733340286331, -3.6375861597263857, -3.6375861597263857, -3.8607297110405954, -3.6375861597263857, -4.553876891600541, -4.197201947661808, -4.6491870714048655, -5.3867860145356445, -3.6375861597263857, -2.9444389791664403, -2.2512917986064953, -2.2512917986064953, -2.9444389791664403, -3.349904087274605, -4.04305126783455, -2.9444389791664403, -4.553876891600541, -4.736198448394496, -4.330733340286331, -2.9444389791664403, -2.9444389791664403, -3.349904087274605, -3.8607297110405954, -3.349904087274605, -2.9444389791664403, -3.349904087274605, -4.197201947661808, -4.04305126783455, -4.736198448394496, -4.736198448394496, -4.6491870714048655, -4.816241156068032, -4.736198448394496, -5.195730777772936, -4.04305126783455, -4.6491870714048655, -2.2512917986064953, -2.9444389791664403, -3.8607297110405954, -2.9444389791664403, -3.349904087274605, -3.349904087274605, -3.349904087274605, -4.197201947661808, -4.04305126783455, -3.8607297110405954, -2.2512917986064953, -2.9444389791664403, -3.349904087274605, -2.2512917986064953, -2.9444389791664403, -3.349904087274605, -3.8607297110405954, -3.8607297110405954, -2.9444389791664403, -3.6375861597263857, -3.349904087274605, -3.8607297110405954, -3.8607297110405954, -3.6375861597263857, -3.8607297110405954, -4.04305126783455, -3.8607297110405954, -2.2512917986064953, -5.0238805208462765, -4.553876891600541, -4.816241156068032, -4.448516375942715, -4.330733340286331, -4.553876891600541, -4.736198448394496, -4.890349128221754, -3.349904087274605, -3.6375861597263857, -4.448516375942715, -2.2512917986064953, -4.6491870714048655, -3.6375861597263857, -4.04305126783455, -4.6491870714048655, -2.9444389791664403, -4.6491870714048655, -3.349904087274605, -4.553876891600541, -3.8607297110405954, -4.330733340286331, -4.736198448394496, -4.448516375942715, -2.2512917986064953, -3.8607297110405954, -3.349904087274605, -4.04305126783455, -4.6491870714048655, -3.8607297110405954, -4.6491870714048655, -2.9444389791664403, -4.197201947661808, -4.04305126783455, -4.197201947661808, -3.8607297110405954, -4.448516375942715, -3.8607297110405954, -4.448516375942715, -4.04305126783455, -3.6375861597263857, -3.6375861597263857, -4.04305126783455, -3.6375861597263857, -4.448516375942715, -4.553876891600541, -4.197201947661808, -4.448516375942715, -3.349904087274605, -4.04305126783455, -4.6491870714048655, -4.6491870714048655, -4.04305126783455, -2.9444389791664403, -4.330733340286331, -4.197201947661808, -4.736198448394496, -2.9444389791664403, -4.6491870714048655, -4.197201947661808, -4.553876891600541, -4.553876891600541, -2.9444389791664403, -2.9444389791664403, -4.330733340286331, -4.6491870714048655, -4.816241156068032, -4.04305126783455, -3.6375861597263857, -3.349904087274605, -4.736198448394496, -3.8607297110405954, -4.197201947661808, -4.6491870714048655, -4.448516375942715, -4.816241156068032, -4.197201947661808, -3.8607297110405954, -3.349904087274605, -4.553876891600541, -4.816241156068032, -3.6375861597263857, -4.04305126783455, -4.6491870714048655, -2.2512917986064953, -4.448516375942715, -3.349904087274605, -4.197201947661808, -4.04305126783455, -3.349904087274605, -4.330733340286331, -4.04305126783455, -3.8607297110405954, -2.2512917986064953, -3.8607297110405954, -4.448516375942715, -4.736198448394496, -4.330733340286331, -2.9444389791664403, -4.197201947661808, -3.349904087274605, -4.04305126783455, -4.04305126783455, -2.9444389791664403, -4.553876891600541, -3.6375861597263857, -4.448516375942715, -4.816241156068032, -4.330733340286331, -4.04305126783455, -3.8607297110405954, -4.553876891600541, -3.8607297110405954, -3.349904087274605, -4.04305126783455, -4.448516375942715, -4.330733340286331, -4.736198448394496, -3.349904087274605, -2.9444389791664403, -4.330733340286331, -4.890349128221754, -4.197201947661808, -4.197201947661808, -1000, -2.2512917986064953, -4.890349128221754, -4.04305126783455, -4.448516375942715, -4.553876891600541, -2.9444389791664403, -4.04305126783455, -4.330733340286331, -4.197201947661808, -2.2512917986064953, -2.9444389791664403, -3.349904087274605, -4.553876891600541, -4.04305126783455, -4.330733340286331, -3.8607297110405954, -3.349904087274605, -2.9444389791664403, -4.04305126783455, -2.9444389791664403, -4.553876891600541, -3.6375861597263857, -3.349904087274605, -3.349904087274605, -4.330733340286331, -2.9444389791664403, -3.8607297110405954, -2.2512917986064953, -3.349904087274605, -2.9444389791664403, -3.6375861597263857, -2.9444389791664403, -4.816241156068032, -4.448516375942715, -4.197201947661808, -4.04305126783455, -4.197201947661808, -3.6375861597263857, -4.736198448394496, -4.553876891600541, -4.553876891600541, -4.04305126783455, -3.6375861597263857, -3.8607297110405954, -3.6375861597263857, -4.197201947661808, -3.8607297110405954, -4.448516375942715, -4.553876891600541, -4.553876891600541, -2.9444389791664403, -4.197201947661808, -3.8607297110405954, -3.8607297110405954, -4.330733340286331, -4.330733340286331, -3.349904087274605, -3.8607297110405954, -3.349904087274605, -3.8607297110405954, -2.2512917986064953, -3.8607297110405954, -4.330733340286331, -4.736198448394496, -2.9444389791664403, -4.04305126783455, -3.8607297110405954, -3.6375861597263857, -4.04305126783455, -4.04305126783455, -4.04305126783455, -3.6375861597263857, -4.816241156068032, -4.330733340286331, -4.448516375942715, -4.448516375942715]\n"
     ]
    }
   ],
   "source": [
    "wp = []\n",
    "sp = []\n",
    "lc = []\n",
    "\n",
    "max_wu = 0\n",
    "max_sp =  0\n",
    "max_lc = -1000\n",
    "\n",
    "for i in range(len(syns_1)):\n",
    "    for j in range(len(syns_1[i])):\n",
    "        for k in range(len(syns_2[i])):\n",
    "            \n",
    "            #print(f\"syn1: {syns_1[i][j]}, syn2: {syns_2[i][k]} --> WU_PALMER: {wu_palmer(syns_1[i][j], syns_2[i][k])} - SHORTEST_PATH: {shortest_path(syns_1[i][j], syns_2[i][k])} - LEAKCOCK_CHODOROW: {leakcock_chodorow(syns_1[i][j], syns_2[i][k])}\")\n",
    "            if wu_palmer(syns_1[i][j], syns_2[i][k]) > float(max_wu):\n",
    "                max_wu = wu_palmer(syns_1[i][j], syns_2[i][k])\n",
    "            if shortest_path(syns_1[i][j], syns_2[i][k]) > float(max_sp):\n",
    "                max_sp = shortest_path(syns_1[i][j], syns_2[i][k])\n",
    "            if leakcock_chodorow(syns_1[i][j], syns_2[i][k]) > float(max_lc):\n",
    "                max_lc = leakcock_chodorow(syns_1[i][j], syns_2[i][k])\n",
    "\n",
    "    wp.append(max_wu) \n",
    "    max_wu = 0\n",
    "    sp.append(max_sp)\n",
    "    max_sp = 0\n",
    "    lc.append(max_lc)\n",
    "    max_lc = -1000\n",
    "\n",
    "print(wp)\n",
    "print(sp)\n",
    "print(lc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Methods\n",
    "- similarity values in WordSim353.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sim_353 = [float(data[2].split('\\n')[0]) for data in dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SPEARMAN'S CORRELATION COEFFICIENT\n",
      "\n",
      "WU & PALMER             0.32927825770618513\n",
      "SHORTEST PATH           0.2236163845432123\n",
      "LEAKCOCK & CHODOROW     0.2236163845432123\n",
      "\n",
      "\n",
      "\n",
      "PEARSON'S CORRELATION COEFFICIENT\n",
      "\n",
      "WU & PALMER             (0.2846480047884611, 5.266550780950281e-08)\n",
      "SHORTEST PATH           (0.08984891365408158, 0.09188617940762013)\n",
      "LEAKCOCK & CHODOROW     (-0.10399156735058393, 0.05091564036689271)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSPEARMAN'S CORRELATION COEFFICIENT\\n\")\n",
    "print(f\"WU & PALMER             {stats.spearmanr(wp, word_sim_353).correlation}\")\n",
    "print(f\"SHORTEST PATH           {stats.spearmanr(sp, word_sim_353).correlation}\")\n",
    "print(f\"LEAKCOCK & CHODOROW     {stats.spearmanr(lc, word_sim_353).correlation}\\n\\n\")\n",
    "\n",
    "print(\"\\nPEARSON'S CORRELATION COEFFICIENT\\n\")\n",
    "print(f\"WU & PALMER             {stats.pearsonr(wp, word_sim_353)}\")\n",
    "print(f\"SHORTEST PATH           {stats.pearsonr(sp, word_sim_353)}\")\n",
    "print(f\"LEAKCOCK & CHODOROW     {stats.pearsonr(lc, word_sim_353)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('spacy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3525effbb77994477fa12acef781f772c9be5a5a62ddcd46b88c81c6046781ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
