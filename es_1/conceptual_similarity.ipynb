{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.nltk.org/howto/wordnet.html WordNet examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_hypernyms(synset):\n",
    "    # get all hypernyms of a synset until the root of wordnet\n",
    "    # estendere per far prendere più iperonimi, non solo il primo ogni volta\n",
    "    ret_list = []\n",
    "    hypernyms = synset.hypernyms()\n",
    "    while hypernyms:\n",
    "        for hyper in hypernyms:\n",
    "            ret_list.append(hyper)\n",
    "        hypernyms = hypernyms[0].hypernyms()\n",
    "    return ret_list\n",
    "\n",
    "def get_all_hypernyms_2(synset):\n",
    "    # get all hypernyms of a synset until the root of wordnet\n",
    "    # estendere per far prendere più iperonimi, non solo il primo ogni volta\n",
    "    ret_list = []\n",
    "    temp_list = []\n",
    "    hypernyms = synset.hypernyms()\n",
    "    while hypernyms:\n",
    "        for hyper in hypernyms:\n",
    "            temp_list.append(hyper)\n",
    "        ret_list.append(temp_list)\n",
    "        temp_list = []\n",
    "        hypernyms = hypernyms[0].hypernyms()\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('whole.n.02')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lowest_common_subsumer(syn1, syn2): \n",
    "    return syn1.lowest_common_hypernyms(syn2)[0] if syn1.lowest_common_hypernyms(syn2) else None\n",
    "\n",
    "def lowest_common_subsumer_2(syn1, syn2):\n",
    "    # risale la gerarchia degli iperonimi, scegliendo sempre e solo il primo synset perchè non ho un PC non della NASA\n",
    "    syn1_hypernyms = get_all_hypernyms(syn1)\n",
    "    syn2_hypernyms = get_all_hypernyms(syn2)\n",
    "\n",
    "    for h1 in syn1_hypernyms:\n",
    "        if h1 in syn2_hypernyms:\n",
    "            return h1\n",
    "\n",
    "lowest_common_subsumer(wn.synsets('car')[0], wn.synsets('dog')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth(syn):\n",
    "    return syn.min_depth() if syn else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_path(): # restituisce sempre 19, per velocizzare le esecuzioni salvo il valore in una costante\n",
    "    max_path = 0\n",
    "    for synset in wn.all_synsets():\n",
    "        if synset.max_depth() > max_path:\n",
    "            max_path = synset.max_depth()\n",
    "    return max_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "11\n",
      "[[Synset('body_part.n.01')], [Synset('part.n.03')], [Synset('thing.n.12')], [Synset('physical_entity.n.01')], [Synset('entity.n.01')]]\n",
      "[[Synset('canine.n.02'), Synset('domestic_animal.n.01')], [Synset('carnivore.n.01')], [Synset('placental.n.01')], [Synset('mammal.n.01')], [Synset('vertebrate.n.01')], [Synset('chordate.n.01')], [Synset('animal.n.01')], [Synset('organism.n.01')], [Synset('living_thing.n.01')], [Synset('whole.n.02')], [Synset('object.n.01')], [Synset('physical_entity.n.01')], [Synset('entity.n.01')]]\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "def my_distance_between_syn(syn_1, syn_2):\n",
    "    if syn_1 == syn_2:\n",
    "        return 0\n",
    "    s1_hyper = get_all_hypernyms_2(syn_1)\n",
    "    s2_hyper = get_all_hypernyms_2(syn_2)\n",
    "\n",
    "    print(s1_hyper)\n",
    "    print(s2_hyper)\n",
    "    s1_temp = get_all_hypernyms(syn_1)\n",
    "    s2_temp = get_all_hypernyms(syn_2)\n",
    "\n",
    "    c_1 = 0\n",
    "    flag = False\n",
    "    for i in range(0, len(s1_hyper)):\n",
    "        for j in range(0, len(s1_hyper[i])):\n",
    "            if s1_hyper[i][j] in s2_temp and not flag:\n",
    "                flag = True\n",
    "                c_1 += i+1\n",
    "\n",
    "    c_2 = 0\n",
    "    flag = False\n",
    "    for i in range(0, len(s2_hyper)):\n",
    "        for j in range(0, len(s2_hyper[i])):\n",
    "            if s2_hyper[i][j] in s1_temp and not flag:\n",
    "                flag = True\n",
    "                c_2 += i+1\n",
    "    \n",
    "   \n",
    "    return (c_1+c_2)\n",
    "        \n",
    "def length(syn1, syn2): # NB, non esistono i cammini tra nomi e verbi in WordNet, pepr cui vanno rimossi i verbi credo\n",
    "    return syn1.shortest_path_distance(syn2) if syn1.shortest_path_distance(syn2) is not None else None\n",
    "\n",
    "\n",
    "print(wn.synsets('chest')[0].shortest_path_distance(wn.synsets('dog')[0]))\n",
    "print(length(wn.synsets('chest')[0], wn.synsets('dog')[0]))\n",
    "print(my_distance_between_syn(wn.synsets('chest')[0], wn.synsets('dog')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPTH_MAX = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('carnivore.n.01')\n",
      "Synset('carnivore.n.01')\n"
     ]
    }
   ],
   "source": [
    "print(lowest_common_subsumer(wn.synset('dog.n.01'), wn.synset('big_cat.n.01')))\n",
    "print(lowest_common_subsumer_2(wn.synset('dog.n.01'), wn.synset('big_cat.n.01')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wu & Palmer\n",
    "Resituisce valori tra 0 e 1, dove più ci si avvicina a 1 più i sensi sono simili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wu_palmer(syn1, syn2):\n",
    "    dep = 0\n",
    "\n",
    "    lcs = lowest_common_subsumer(syn1, syn2)\n",
    "    dep = (depth(syn1) + depth(syn2))\n",
    "    \n",
    "    if dep == 0:\n",
    "        dep = 0.001\n",
    "        \n",
    "    return 2 * depth(lcs) / dep\n",
    "    #return syn1.wup_similarity(syn2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortest Path\n",
    "Valore che può oscillare tra 0 e 2depthMax.\n",
    "Più il valore si avvicina a 2depthMax più i sensi sono simili, questo perchè vuol dire che la distanza tra i due sensi (len(s1,s2)) è minima o uguale a 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_path(syn1, syn2):\n",
    "    #return syn1.path_similarity(syn2) \n",
    "    return 2 * DEPTH_MAX - length(syn1, syn2) if length(syn1, syn2) is not None else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leakcock & Chodorow\n",
    "I valori sono compresi tra 0 e log(2depthMax + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.6888794541139363\n",
      "2.0281482472922856\n"
     ]
    }
   ],
   "source": [
    "def leakcock_chodorow(syn1, syn2):\n",
    "    '''if syn1.name().split('.')[1] == syn2.name().split('.')[1]:\n",
    "        return syn1.lch_similarity(syn2)\n",
    "    else:\n",
    "        return 0'''\n",
    "    distance = length(syn1, syn2) \n",
    "    if distance is not None:\n",
    "        if distance != 0:\n",
    "            return -math.log(distance / 2 * DEPTH_MAX)\n",
    "        else:\n",
    "             return -math.log(distance+1 / (2 * DEPTH_MAX)+1)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "print(leakcock_chodorow(wn.synsets('dog')[0], wn.synsets('cat')[0]))\n",
    "print(wn.synsets('dog')[0].lch_similarity(wn.synsets('cat')[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read lines from WordSim353.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "with open(r'../resources/WordSim353.csv', 'r') as f:\n",
    "    word_sim = f.readlines()[1:]\n",
    "    for tuple in word_sim:\n",
    "        dataset.append(tuple.split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get synset from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns_1 = []\n",
    "syns_2 = []\n",
    "\n",
    "for tuple in dataset:\n",
    "    syns_1.append(wn.synsets(tuple[0]))\n",
    "    syns_2.append(wn.synsets(tuple[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute similarity using the three methods described above over all the combinations of synsets of every word in the input file.\n",
    "For each couple, take the maximum value of each similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353\n",
      "353\n",
      "353\n"
     ]
    }
   ],
   "source": [
    "wp = []\n",
    "sp = []\n",
    "lc = []\n",
    "\n",
    "max_wu = 0\n",
    "max_sp =  0\n",
    "max_lc = 0\n",
    "\n",
    "for i in range(len(syns_1)):\n",
    "    for j in range(len(syns_1[i])):\n",
    "        for k in range(len(syns_2[i])):\n",
    "            \n",
    "            #print(f\"syn1: {syns_1[i][j]}, syn2: {syns_2[i][k]} --> WU_PALMER: {wu_palmer(syns_1[i][j], syns_2[i][k])} - SHORTEST_PATH: {shortest_path(syns_1[i][j], syns_2[i][k])} - LEAKCOCK_CHODOROW: {leakcock_chodorow(syns_1[i][j], syns_2[i][k])}\")\n",
    "            \n",
    "            wu_temp = wu_palmer(syns_1[i][j], syns_2[i][k])\n",
    "            sp_temp = shortest_path(syns_1[i][j], syns_2[i][k])\n",
    "            lc_temp = leakcock_chodorow(syns_1[i][j], syns_2[i][k])\n",
    "\n",
    "            if wu_temp > float(max_wu):\n",
    "                max_wu = wu_temp\n",
    "            if sp_temp > float(max_sp):\n",
    "                max_sp = sp_temp\n",
    "            if lc_temp < float(max_lc):\n",
    "                max_lc = lc_temp\n",
    "\n",
    "    wp.append(max_wu) \n",
    "    max_wu = 0\n",
    "    sp.append(max_sp)\n",
    "    max_sp = 0\n",
    "    lc.append(max_lc)\n",
    "    max_lc = 0\n",
    "\n",
    "print(len(wp))\n",
    "print(len(sp))\n",
    "print(len(lc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Methods\n",
    "- similarity values in WordSim353.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sim_353 = [float(data[2].strip('\\n')) for data in dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SPEARMAN'S CORRELATION COEFFICIENT\n",
      "\n",
      "WU & PALMER             0.32927825770618513\n",
      "SHORTEST PATH           0.2895253335747299\n",
      "LEAKCOCK & CHODOROW     0.10503987463606122\n",
      "\n",
      "\n",
      "\n",
      "PEARSON'S CORRELATION COEFFICIENT\n",
      "\n",
      "WU & PALMER             (0.2846480047884611, 5.266550780950281e-08)\n",
      "SHORTEST PATH           (0.16653216769600956, 0.0016911511526584368)\n",
      "LEAKCOCK & CHODOROW     (0.15199780024041876, 0.004205795923151336)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSPEARMAN'S CORRELATION COEFFICIENT\\n\")\n",
    "print(f\"WU & PALMER             {stats.spearmanr(wp, word_sim_353).correlation}\")\n",
    "print(f\"SHORTEST PATH           {stats.spearmanr(sp, word_sim_353).correlation}\")\n",
    "print(f\"LEAKCOCK & CHODOROW     {stats.spearmanr(lc, word_sim_353).correlation}\\n\\n\")\n",
    "\n",
    "print(\"\\nPEARSON'S CORRELATION COEFFICIENT\\n\")\n",
    "print(f\"WU & PALMER             {stats.pearsonr(wp, word_sim_353)}\")\n",
    "print(f\"SHORTEST PATH           {stats.pearsonr(sp, word_sim_353)}\")\n",
    "print(f\"LEAKCOCK & CHODOROW     {stats.pearsonr(lc, word_sim_353)}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('spacy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3525effbb77994477fa12acef781f772c9be5a5a62ddcd46b88c81c6046781ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
